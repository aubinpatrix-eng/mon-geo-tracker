import streamlit as st
import google.generativeai as genai
import pandas as pd
import json

# --- CONFIGURATION ---
st.set_page_config(page_title="GEO Tracker (Mode Grounding)", layout="wide")

st.title("üåç GEO Analytics Tracker (avec Google Search)")
st.markdown("""
**Moteur :** Gemini 1.5 Flash + **Google Search Grounding**.
**Ce qui change :** L'IA va chercher sur le vrai web pour r√©pondre. On peut donc voir **QUELS SITES** te citent.
""")

# --- SIDEBAR ---
with st.sidebar:
    st.header("Param√®tres")
    api_key = st.text_input("Ta cl√© API Google (AI Studio)", type="password")
    
    # On garde Flash, c'est le meilleur ratio vitesse/gratuit
    model_name = "gemini-1.5-flash"

if api_key:
    try:
        genai.configure(api_key=api_key)
    except Exception as e:
        st.error(f"Erreur cl√© API : {e}")
        st.stop()
else:
    st.warning("Entre ta cl√© API Google pour commencer.")
    st.stop()

# --- INPUTS ---
col1, col2 = st.columns(2)
with col1:
    target_brand = st.text_input("Ta Marque", value="Nike")
with col2:
    competitors = st.text_input("Concurrents", value="Adidas, Asics")

input_questions = st.text_area(
    "Questions (Simulations)", 
    value="Quelle est la meilleure chaussure de running ?\nTop 3 marques de sport pour le marathon"
)

start_btn = st.button("Lancer l'Audit GEO (Live Web)", type="primary")

# --- FONCTIONS INTELLIGENTES ---

def get_gemini_search_response(question):
    """
    SIMULATEUR : Utilise l'outil Google Search pour r√©pondre avec des faits r√©els.
    """
    try:
        # On active l'outil de recherche Google (Grounding)
        tools = 'google_search_retrieval'
        model = genai.GenerativeModel(model_name, tools=tools)
        
        # On force l'IA √† explicitement montrer ses sources dans le texte
        prompt = f"""
        Agis comme un moteur de recherche IA avanc√©.
        Question : {question}
        
        Consignes :
        1. Fais une recherche Google pour trouver des informations r√©centes.
        2. R√©ponds √† la question de mani√®re utile pour l'utilisateur.
        3. IMPORTANT : √Ä la fin, liste explicitement les URL des sources que tu as utilis√©es.
        """
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Erreur Grounding : {e}"

def analyze_response_with_sources(llm_answer, brand):
    """
    JUGE : Analyse le texte pour trouver la marque ET les sources (URL)
    """
    generation_config = {"response_mime_type": "application/json"}
    model_judge = genai.GenerativeModel(model_name, generation_config=generation_config)
    
    prompt = f"""
    Tu es un analyste de donn√©es. Analyse la r√©ponse IA ci-dessous.
    
    Marque cible : "{brand}"
    
    R√©ponds avec ce JSON exact :
    {{
        "cited": boolean, (La marque est-elle cit√©e ?)
        "sentiment": string, (Positif/Neutre/N√©gatif)
        "sources_urls": list of strings, (Extrais toutes les URLs ou noms de domaine cit√©s dans le texte qui recommandent ou parlent du sujet)
        "rank_impression": integer
    }}
    
    Texte √† analyser :
    \"\"\"{llm_answer}\"\"\"
    """
    
    try:
        response = model_judge.generate_content(prompt)
        return json.loads(response.text)
    except Exception as e:
        return {"cited": False, "sentiment": "Error", "sources_urls": [], "rank_impression": 0}

# --- MAIN LOOP ---

if start_btn:
    questions_list = [q.strip() for q in input_questions.split('\n') if q.strip()]
    results = []
    
    progress_bar = st.progress(0)
    st_status = st.status("Recherche Google en cours...", expanded=True)
    
    for i, question in enumerate(questions_list):
        st_status.write(f"üåç Recherche pour : {question}")
        
        # 1. Appel avec Grounding (Recherche Web r√©elle)
        llm_text = get_gemini_search_response(question)
        
        # 2. Analyse
        analysis = analyze_response_with_sources(llm_text, target_brand)
        
        # Nettoyage des sources pour l'affichage (on garde juste les domaines parfois c'est plus propre)
        sources_clean = ", ".join(analysis.get('sources_urls', [])[:3]) # On garde les 3 premi√®res
        
        row = {
            "Question": question,
            "Pr√©sence": "‚úÖ" if analysis.get('cited') else "‚ùå",
            "Sources (Influenceurs)": sources_clean, # NOUVEAU
            "Sentiment": analysis.get('sentiment'),
            "R√©ponse Compl√®te": llm_text 
        }
        results.append(row)
        progress_bar.progress((i + 1) / len(questions_list))

    st_status.update(label="Audit Termin√© !", state="complete", expanded=False)

    # --- RESULTS ---
    st.divider()
    if results:
        df = pd.DataFrame(results)
        
        # Affichage du tableau
        st.subheader("R√©sultats avec Sources Identifi√©es")
        st.dataframe(
            df[["Question", "Pr√©sence", "Sentiment", "Sources (Influenceurs)"]], 
            use_container_width=True
        )
        
        # Petit tuto d'interpr√©tation
        st.info("üí° **Astuce GEO :** La colonne 'Sources' te montre les sites que l'IA a lus pour construire sa r√©ponse. Si tu veux √™tre cit√© par l'IA, tu dois obtenir des articles ou des liens sur ces sites pr√©cis (C'est √ßa, le GEO !).")

        with st.expander("Voir les r√©ponses compl√®tes"):
            for r in results:
                st.markdown(f"**Q: {r['Question']}**")
                st.markdown(r['R√©ponse Compl√®te'])
                st.divider()
